# 简单神经网络运算框架的实现

## 代码地址：



## 模块结构

### train:

载入数据，训练神经网络

将测试数据以`.mat` 的形式读入并进行训练

### layers：

定义各层神经网络

数据输入层`class data`: 

​	`get_data`  获取数据	`shuffle`  置乱数据	`pull_data`   将数据推向输出	

全连接层`class_connected_layer` :

​	`forward`  前向传播

```python
def forward(self):
        self.outputs = self.inputs.dot(self.weights) + np.tile(self.bias,(batch_size, 1))
```

​	`backward`  反向传播

```python
def backward(self):
    # 求权值的梯度,求得的结果是一个三维的数组,因为有多个样本;
    for i in np.arange(batch_size):
        self.grad_weights[i, :] = np.tile(self.inputs[i, :], (1, 1)).T \
        .dot(np.tile(self.grad_outputs[i, :], (1, 1))) + \
    	self.weights * weights_decay
    # 求求偏置的梯度;
    self.grad_bias = self.grad_outputs
    # 求输入的梯度;
    self.grad_inputs = self.grad_outputs.dot(self.weights.T)
```

​	`update`  更新权值

```python
def update(self):
# 权值与偏置的更新;
    grad_weights_average = np.mean(self.grad_weights, 0)
    grad_bias_average = np.mean(self.grad_bias, 0)
    (self.weights, self.weights_previous_direction) = update_function(self.weights,
    grad_weights_average,
    self.weights_previous_direction)
    (self.bias, self.bias_previous_direction) = update_function(self.bias,
    grad_bias_average,
    self.bias_previous_direction)
```

激活层`class activation_layer`:

​	激活函数`sigmoid/tanh/relu`

​	`	forward`  前向传播	`backward`  反向传播

损失函数层`class loss_layer`:

​	` softmax`  

批向量化层`class batch_normalization`:

​	`forward`  前向传播

```python
def forward(self):
    momentum = 0.9
    eps = 1e-5
    self.mu_mean = np.mean(self.inputs)
    self.mu_var = np.var(self.inputs)
    self.inputs_hat = (self.inputs - self.mu_mean) / np.sqrt(self.mu_var + eps)
    self.running_mean = np.zeros(self.inputs.shape[1])
    self.running_var = np.zeros(self.inputs.shape[1])
    self.running_mean = momentum * self.running_mean + (1 - momentum) * self.mu_mean
    self.running_var = momentum * self.running_var + (1 - momentum) * self.mu_var
    self.test = self.inputs / np.sqrt(self.running_var + eps) - self.running_mean / np.sqrt(self.running_var + eps)
```

​	`backward`  反向传播

```python
def backward(self):
    momentum = 0.9
    eps = 1e-5
    dout_ = self.grad_inputs
    dvar = np.sum(dout_ * (self.inputs - self.mu_mean) * -0.5 * (self.mu_var + eps) ** -1.5, axis=0)
    dx_ = 1 / np.sqrt(self.mu_var + eps)
    dvar_ = 2 * (self.inputs - self.mu_mean) / self.grad_inputs.shape[0]

    # intermediate for convenient calculation
    di = dout_ * dx_ + dvar * dvar_
    dmean = -1 * np.sum(di, axis=0)
    dmean_ = np.ones_like(self.inputs) / self.grad_inputs.shape[0]
    self.dx = di + dmean * dmean_
```

### function_for_layer:

定义激活函数，损失函数: `relu/tanh/sigmoid`	`softmax`

初始化方法：`xavier`  用于超参数随机化搜索的几个分布

### update_method：

定义学习率及权值的更新机制

`batch_gradient_descent` 基于批量的随机梯度下降法

### net：

定义神经网络

`全连接层` ->`bn层`->`relu` ->`全连接层`->`bn层` ->`relu`->`全连接层`  ->`softmax`

### train:

手写体数字简介：来自[Yann LeCun](http://yann.lecun.com/exdb/mnist/index.html) 等人维护的一个手写数字集，以`.mat`格式读入

训练数据如下：

```python
num_train = 2000
lr = 0.1
weight_decay = 0.001
train_batch_size = 100
test_batch_size = 10000
```

## 运行结果

loss - iteration

![Figure_1](C:\Users\Marlowe\Desktop\Figure_1.png)

` 测试样本的识别率为: 0.9606`

## reference

[github cs231n笔记](https://github.com/Sampson-Lee/notes4cs231n)

[Batch Normalization学习笔记](https://zhuanlan.zhihu.com/p/26138673)

[用于超参数随机化搜索的几个分布](http://www.mamicode.com/info-detail-2594101.html)

[深层神经网络框架的python实现](http://www.demodashi.com/demo/13010.html)



































